---
title: "Assignment4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Metropolis-Hastings

**Section 1**

Denote $\theta^\ast$ and $\theta^{(t-1)}$ as the proposed candidate and the old value, respectively. Let $J_t(x|y) = \beta(cy,c(c-y))$ be the jumping function, and $P(x)$ be the probability density function we need to sample on. The acceptance ratio is:

$$r = \frac{P(\theta^\ast)}{P(\theta^{(t-1)})} \frac{J_t(\theta^{(t-1)}|\theta^\ast)}{J_t(\theta^\ast|\theta^{(t-1)})}$$

Expanding the first factor using the formula for beta distribution gives:

$$r = \left(\frac{\theta^\ast}{\theta^{(t-1)}}\right)^{6-1} \left(\frac{1-\theta^\ast}{1-\theta^{(t-1)}}\right)^{4-1} \frac{J_t(\theta^{(t-1)}|\theta^\ast)}{J_t(\theta^\ast|\theta^{(t-1)})}$$

To implement the Metropolis-Hastings (MH) algorithm, we first initialize a vector for the draws. The first value of this vector is the starting value, which is generated from a uniform distribution between 0 and 1 (because beta distribution is only applicable for $0 \leq x \leq 1$). Then, we draw a new proposal $\theta^\ast$ candidate from $J_t$, and substitute it and the previous value $\theta^{(t-1)}$ in the vector into the formula for the acceptance ratio $r$. Finally, we accept $\theta^\ast$ with probability $\min(r,1)$, which can be done by an if condition. (Randomly generate a value between 0 and 1, and accept $\theta^\ast$ if this value is less than or equal to $\min(r,1)$; if not, we accept $\theta^{(t-1)}$.)

```{r}
#---------- Metropolis-Hastings sampling ----------#
myMH <- function(c, nIter, burnIn = 0){
  ### c: scaling factor
  ### nIter: number of iterations
  ### burnIn: discarded values of the chain
  
  x0 <- runif(1, min=0, max=1) # Randomly generate the starting value between 0 and 1
  
  chain <- numeric(nIter+1) # Initialize result vector
  chain[1] <- x0
  
  for (ii in 1:nIter){
    
    # Draw new candidate from beta function
    x1 <- rbeta(1, shape1 = c*chain[ii], shape2 = c*(1-chain[ii]))
    
    # Acceptance ratio
    r <- (x1/chain[ii])^5*((1-x1)/(1-chain[ii]))^3*dbeta(chain[ii], shape1 = c*x1, shape2 = c*(1-x1))/dbeta(x1, shape1 = c*chain[ii], shape2 = c*(1-chain[ii]))
    
    if (runif(1, min=0, max=1) <= min(r,1)) {chain[ii+1] <- x1} else {chain[ii+1] <- chain[ii]}
  }
  
  # Get rid of burn in values
  if (burnIn > 0) {chain <- chain[-(1:burnIn)]}
  
  return(chain)
}

draws1 <- myMH(1,10000,0) # c = 1, no thinning
  
# Plot
par(mfrow=c(1,3))  #1 row, 3 columns
plot(draws1); acf(draws1); hist(draws1, xlim=c(0, 1))  #plot commands
```


**Section 2**

```{r}
#--------- Compare with beta(6,4) ----------#
draws.true <- rbeta(10000,6,4)
hist(draws1, xlim=c(0, 1), col = rgb(0,0,1,.5), xlab = "x")
hist(draws.true, col = rgb(1,0,0,.5), add = T)
legend("topleft", c("MH sampling", "True Beta(6,4)"), col=c(rgb(0,0,1,.5), rgb(1,0,0,.5)), lwd = 4, inset = c(0,0.1))


#---------- Kolmogorov-Smirnov test ----------#
ks.test(draws1,draws.true)
```

**Section 3**

We now perform the sampling with different c values (0.1, 2.5, and 10) with 10000 draws and 2000 burn-ins.

```{r}
#---------- Different sampling settings ----------#
draws2 <- myMH(.1,10000,2000)
draws3 <- myMH(2.5,10000,2000)
draws4 <- myMH(10,10000,2000)

par(mfrow=c(1,3))  #1 row, 3 columns
plot(draws2); acf(draws2); hist(draws2, xlim=c(0, 1))  #plot commands
par(mfrow=c(1,3))  #1 row, 3 columns
plot(draws3); acf(draws3); hist(draws3, xlim=c(0, 1))  #plot commands
par(mfrow=c(1,3))  #1 row, 3 columns
plot(draws4); acf(draws4); hist(draws4, xlim=c(0, 1))  #plot commands
```

To take into account the difference in the sampling size, we compare the samplers with the built-in beta distribution function in term of probability density instead of the frequency. The densities are shown below.

```{r}
## Compare histograms separately in term of probability density
plot(density(draws1), col = "blue", xlim=c(0, 1), ylim=c(0,3.5), main = "", xlab = expression(phi), ylab = "PDF", lwd = 2)
lines(density(draws2), col = "green", lwd = 2)
lines(density(draws3), col = "black", lwd = 2)
lines(density(draws4), col = "purple", lwd = 2)
lines(density(draws.true), col = "red", lwd = 4, lty = 2)
legend("topleft", c("c = 1.0","c = 0.1","c = 2.5","c = 10","True Beta(6,4)"), col=c("blue","green","black","purple","red"), lty = c(1,1,1,1,2), lwd = 3)
title(main = "Probability density from MH sampling versus true density")
```

Judging from the plot, c = 10 seems to give the best results, and the sampling worsen with lower values of c. At c = 0.1, the MH algorithm returns unacceptable results. This proves the importance of the choice of the jumping function.

## Gibbs sampling

Marginal distribution:

$$p(x|y) = C_0 y e^{-yx}$$

First, we need to normalize $p(x,y)$ for $0<x<B$ to find $C_0$:

$$\int_0^B p(x,y)~dx = 1$$
$$\left.C_0 y \frac{e^{-yx}}{-y}\right|_{x=0}^{x=B} = 1$$
$$C_0 = \frac{1}{1-e^{-By}}$$

Next, we calculate the cumulative distribution function $P(x)$:

$$P(x) = \int_0^x p(t,y)~dt = \frac{1}{1-e^{-By}}(1-e^{-xy})$$

The inverse cumulative distribution function is:

$$x = P^{-1}(y) = -\frac{1}{y}\ln{\left[1-u(1-e^{-By})\right]}$$
where u is drawn from a uniform distribution between 0 and 1. Similarly:

$$y = P^{-1}(x) = -\frac{1}{x}\ln{\left[1-u(1-e^{-Bx})\right]}$$

```{python}
##### Import libraries
import numpy as np
import matplotlib.pyplot as plt

from math import *
from random import *

##### Define inverse CDF function for x|y
def CDF_xy(y, B = 5.0):
    P = uniform(0,1)
    x = -1/y*log(1 - P*(1-exp(-y*B)))
    return(x)

def myGibbs(T, B = 5.0):
    # Initialize a matrix of 2 columns for x and y
    chain = np.zeros((T,2))
    chain[0,0] = uniform(0,B)
    chain[0,1] = uniform(0,B)

    for ii in range(0,T-1):
        chain[ii+1,0] = CDF_xy(chain[ii,1], B)
        chain[ii+1,1] = CDF_xy(chain[ii,0], B)

    return(chain)

##### Sampling
# Set B = 5
myB = 5.0

# Sampling 500 points
mychain = myGibbs(500, myB)
myx1 = mychain[:,0]


plt.hist(myx1, bins = np.linspace(0,myB,20))
plt.title("Gibbs sampling for 500 data points")
plt.xlabel("Value")
plt.ylabel("Frequency")
plt.savefig("gibbs500.png") # save figure externally

mychain = myGibbs(5000, myB)
myx2 = mychain[:,0]
mychain = myGibbs(50000, myB)
myx3 = mychain[:,0]

##### Expectation of x
print "Expectation from 500 samples is %.6f" % np.mean(myx1)
print "Expectation from 5000 samples is %.6f" % np.mean(myx2)
print "Expectation from 50000 samples is %.6f" % np.mean(myx3)
```

The histogram plot for 500 samples is saved externally as `gibbs500.png`.