---
title: "Assignment4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Metropolis-Hastings

**Section 1**

Denote $\theta^\ast$ and $\theta^{(t-1)}$ as the proposed candidate and the old value, respectively. Let $J_t(x|y) = \beta(cy,c(c-y))$ be the jumping function, and $P(x)$ be the probability density function we need to sample on. The acceptance ratio is:

$$r = \frac{P(\theta^\ast)}{P(\theta^{(t-1)})} \frac{J_t(\theta^{(t-1)}|\theta^\ast)}{J_t(\theta^\ast|\theta^{(t-1)})}$$

Expanding the first factor using the formula for beta distribution gives:

$$r = \left(\frac{\theta^\ast}{\theta^{(t-1)}}\right)^{6-1} \left(\frac{1-\theta^\ast}{1-\theta^{(t-1)}}\right)^{4-1} \frac{J_t(\theta^{(t-1)}|\theta^\ast)}{J_t(\theta^\ast|\theta^{(t-1)})}$$

To implement the Metropolis-Hastings (MH) algorithm, we first initialize a vector for the draws. The first value of this vector is the starting value, which is generated from a uniform distribution between 0 and 1 (because beta distribution is only applicable for $0 \leq x \leq 1$). Then, we draw a new proposal $\theta^\ast$ candidate from $J_t$, and substitute it and the previous value $\theta^{(t-1)}$ in the vector into the formula for the acceptance ratio $r$. Finally, we accept $\theta^\ast$ with probability $\min(r,1)$, which can be done by an if condition. (Randomly generate a value between 0 and 1, and accept $\theta^\ast$ if this value is less than or equal to $\min(r,1)$; if not, we accept $\theta^{(t-1)}$.)

```{r}
#---------- Metropolis-Hastings sampling ----------#
myMH <- function(c, nIter, burnIn = 0){
  ### c: scaling factor
  ### nIter: number of iterations
  ### burnIn: discarded values of the chain
  
  chain <- numeric(nIter+1) # Initialize result vector
  chain[1] <- runif(1, min=0, max=1) # Randomly generate the starting value between 0 and 1
  
  for (ii in 1:nIter){
    # Draw new candidate from beta function
    x1 <- rbeta(1, shape1 = c*chain[ii], shape2 = c*(1-chain[ii]))
    
    # Acceptance ratio
    r <- (x1/chain[ii])^5*((1-x1)/(1-chain[ii]))^3*dbeta(chain[ii], shape1 = c*x1, shape2 = c*(1-x1))/dbeta(x1, shape1 = c*chain[ii], shape2 = c*(1-chain[ii]))
    
    if (runif(1, min=0, max=1) <= min(r,1)) {chain[ii+1] <- x1} else {chain[ii+1] <- chain[ii]}
  }
  
  # Get rid of burn in values
  if (burnIn > 0) {chain <- chain[-(1:burnIn)]}
  
  return(chain)
}

draws1 <- myMH(1,10000,0) # c = 1, no thinning
  
# Plot
par(mfrow=c(1,3))  #1 row, 3 columns
plot(draws1); acf(draws1); hist(draws1, xlim=c(0, 1))  #plot commands
```


**Section 2**

```{r}
#--------- Compare with beta(6,4) ----------#
draws.true <- rbeta(10000,6,4)
hist(draws1, xlim=c(0, 1), col = rgb(0,0,1,.5), xlab = "x")
hist(draws.true, col = rgb(1,0,0,.5), add = T)
legend("topleft", c("MH sampling", "True Beta(6,4)"), col=c(rgb(0,0,1,.5), rgb(1,0,0,.5)), lwd = 4, inset = c(0,0.1))


#---------- Kolmogorov-Smirnov test c = 1 ----------#
ks.test(draws1,draws.true)
```

In a 2-sample K-S test, a high p-value (ie, more than 0.05) means the samples are drawn from the same distribution.

At c = 1, the sampler is not reliable: the agreement between the MH sampler and the true beta distribution is consistent. Over many runs, the p-value is usually, but not always, less than 0.05, suggesting that the sampler does not accurately follow the true beta distribution. This might be because the sample size and thinning is not optimal, and we will check this possibility in the next section.

**Section 3**

We now perform the sampling with different c values (0.1, 1, 2.5, and 10) with 10000 draws and 5000 burn-ins.

```{r}
#---------- Different sampling settings ----------#
draws1 <- myMH(1,10000,5000)
draws2 <- myMH(.1,10000,5000)
draws3 <- myMH(2.5,10000,5000)
draws4 <- myMH(10,10000,5000)

par(mfrow=c(1,3))  #1 row, 3 columns
plot(draws2); acf(draws2); hist(draws2, xlim=c(0, 1))  #plot commands
par(mfrow=c(1,3))  #1 row, 3 columns
plot(draws3); acf(draws3); hist(draws3, xlim=c(0, 1))  #plot commands
par(mfrow=c(1,3))  #1 row, 3 columns
plot(draws4); acf(draws4); hist(draws4, xlim=c(0, 1))  #plot commands
```

To take into account the difference in the sampling size, we compare the samplers with the built-in beta distribution function in term of probability density instead of the frequency. The densities are shown below.

```{r}
## Compare histograms separately in term of probability density
plot(density(draws1), col = "blue", xlim=c(0, 1), ylim=c(0,3.5), main = "", xlab = expression(phi), ylab = "PDF", lwd = 2)
lines(density(draws2), col = "green", lwd = 2)
lines(density(draws3), col = "black", lwd = 2)
lines(density(draws4), col = "purple", lwd = 2)
lines(density(draws.true), col = "red", lwd = 4, lty = 2)
legend("topleft", c("c = 1.0","c = 0.1","c = 2.5","c = 10","True Beta(6,4)"), col=c("blue","green","black","purple","red"), lty = c(1,1,1,1,2), lwd = 3)
title(main = "Probability density from MH sampling versus true density")
```

Judging from the plot, c = 10 seems to give the best results, and the sampling worsen with lower values of c. At c = 0.1, the MH algorithm returns unacceptable results. However, the K-S test suggests that c = 2.5 is usually the better sampler, at least with 10000 draws and 5000 burn-ins:  

```{r}
#---------- Kolmogorov-Smirnov test c = 1 ----------#
ks.test(draws1,draws.true)

#---------- Kolmogorov-Smirnov test c = 0.1 ----------#
ks.test(draws2,draws.true)

#---------- Kolmogorov-Smirnov test c = 2.5 ----------#
ks.test(draws3,draws.true)

#---------- Kolmogorov-Smirnov test c = 10 ----------#
ks.test(draws4,draws.true)
```

Note that, because of the random nature of the algorithm, the p-value for c = 2.5 above might be less than 0.05, depending on each individual run.

We have also shown 2 different draws with c = 1, with and without thinning. As we can see, the sampler is still inaccurate, regradless of thinning.

We will now check the sampler with 100000 draws and 5000 burn-ins, which seem to be the optimal parameters. Here is the result for the K-S tests:

```{r}
#---------- Kolmogorov-Smirnov test c = 1 ----------#
draws1b <- myMH(1,100000,5000)
ks.test(draws1b,draws.true)

#---------- Kolmogorov-Smirnov test c = 0.1 ----------#
draws2b <- myMH(.1,100000,5000)
ks.test(draws2b,draws.true)

#---------- Kolmogorov-Smirnov test c = 2.5 ----------#
draws3b <- myMH(2.5,100000,5000)
ks.test(draws3b,draws.true)

#---------- Kolmogorov-Smirnov test c = 10 ----------#
draws4b <- myMH(10,100000,5000)
ks.test(draws4b,draws.true)
```

The c = 1 sampler has improved in both accuracy and consistency, while the accuracy of c = 0.1 and 10 still depend on each individual run. Interestingly, c = 1 is usually more accurate (ie, higher p-value) than c = 2.5. Nevertheless, while c = 2.5 is the most consistent value for c, it still suffers from the randomness of the algorithm.

The comparison above shows the importance of the choice of the jumping function (eg, c = 0.1 is not suitable), as well as the need for sampling size and thinning (most apparent for c = 1). Over many runs, we have also noticed that there will always be at least one run where the MH algorithm cannot capture the beta distribution.

## Gibbs sampling

Marginal distribution:

$$p(x|y) = C_0 y e^{-yx}$$

First, we need to normalize $p(x,y)$ for $0<x<B$ to find $C_0$:

$$\int_0^B p(x,y)~dx = 1$$
$$\left.C_0 y \frac{e^{-yx}}{-y}\right|_{x=0}^{x=B} = 1$$
$$C_0 = \frac{1}{1-e^{-By}}$$

Next, we calculate the cumulative distribution function $P(x)$:

$$P(x) = \int_0^x p(t,y)~dt = \frac{1}{1-e^{-By}}(1-e^{-xy})$$

The inverse cumulative distribution function is:

$$x = P^{-1}(y) = -\frac{1}{y}\ln{\left[1-u(1-e^{-By})\right]}$$
where u is drawn from a uniform distribution between 0 and 1. Similarly:

$$y = P^{-1}(x) = -\frac{1}{x}\ln{\left[1-u(1-e^{-Bx})\right]}$$

```{python}
##### Import libraries
import numpy as np
import matplotlib.pyplot as plt

from math import *
from random import *

##### Define inverse CDF function for x|y
def CDF_xy(y, B = 5.0):
    P = uniform(0,1)
    x = -1/y*log(1 - P*(1-exp(-y*B)))
    return(x)

def myGibbs(T, B = 5.0):
    # Initialize a matrix of 2 columns for x and y
    chain = np.zeros((T,2))
    chain[0,0] = uniform(0,B)
    chain[0,1] = uniform(0,B)

    for ii in range(0,T-1):
        chain[ii+1,0] = CDF_xy(chain[ii,1], B)
        chain[ii+1,1] = CDF_xy(chain[ii,0], B)

    return(chain)

##### Sampling
# Set B = 5
myB = 5.0

# Sampling 500 points
mychain = myGibbs(500, myB)
myx1 = mychain[:,0]


plt.hist(myx1, bins = np.linspace(0,myB,20))
plt.title("Gibbs sampling for 500 data points")
plt.xlabel("Value")
plt.ylabel("Frequency")
plt.savefig("gibbs500.png") # save figure externally

mychain = myGibbs(5000, myB)
myx2 = mychain[:,0]
mychain = myGibbs(50000, myB)
myx3 = mychain[:,0]

##### Expectation of x
print "Expectation from 500 samples is %.6f" % np.mean(myx1)
print "Expectation from 5000 samples is %.6f" % np.mean(myx2)
print "Expectation from 50000 samples is %.6f" % np.mean(myx3)
```

The histogram plot for 500 samples is saved externally as `gibbs500.png`.